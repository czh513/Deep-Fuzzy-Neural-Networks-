{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qGii2OM3-i8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.randn(1000, 9)\n",
    "train_y = torch.randint(3, size=(1000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = 0.1\n",
    "\n",
    "class AbsLinear(nn.Linear):\n",
    "    ''' A linear module that always applies abs() on the weight '''\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight.abs(), self.bias)\n",
    "    \n",
    "class Elliptical(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Elliptical, self).__init__(*args, **kwargs)\n",
    "        kwargs['bias'] = False\n",
    "        self._quadratic = AbsLinear(*args, **kwargs)\n",
    "    def forward(self, input):\n",
    "        linear_term = super(Elliptical, self).forward(input)\n",
    "        quadratic_term = self._quadratic.forward(input*input)\n",
    "        return -multiplier * quadratic_term + linear_term\n",
    "\n",
    "def plot_model(model):\n",
    "    val_x = np.linspace(0, 1, 100)\n",
    "    val_y = np.linspace(0, 1, 100)\n",
    "    val_x, val_y = np.meshgrid(val_x, val_y)\n",
    "    input = np.stack([val_x.flatten(), val_y.flatten()], axis=-1)\n",
    "    val_z = model(torch.from_numpy(input).float()).detach().numpy().reshape(val_x.shape)\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    ax.contourf(val_x, val_y, val_z, 10, cmap=plt.cm.bone, origin='lower')\n",
    "\n",
    "def build_model(use_elliptical=False):\n",
    "    layers = []\n",
    "    for _ in range(20):\n",
    "        layers.extend([\n",
    "            Elliptical(9, 9) if use_elliptical else nn.Linear(9, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(9)\n",
    "        ])\n",
    "    return nn.Sequential(*(layers + [nn.Linear(9, 3)]))\n",
    "    \n",
    "def demo_model(use_elliptical=False, n_epochs=100, lr=0.01, report_interval=10):\n",
    "    model = build_model(use_elliptical)\n",
    "    # train\n",
    "    lr = 0.5\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    plt.ion()\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(train_x)\n",
    "        loss = loss_func(preds, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if report_interval > 0 and epoch % report_interval == 0:\n",
    "            _, pred_y = torch.max(preds, dim=1)\n",
    "            print('Acc:', (pred_y == train_y).float().mean().item())\n",
    "#             plot_model(model)\n",
    "#     plot_model(model)\n",
    "    plt.ioff()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0221, -0.0720],\n",
       "        [ 0.0463,  0.6099]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weight(m):\n",
    "    if isinstance(m, Elliptical):\n",
    "        return torch.cat([m.weight, m._quadratic.weight])\n",
    "    else:\n",
    "        return m.weight\n",
    "    \n",
    "get_weight(Elliptical(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.33899998664855957\n",
      "Acc: 0.32600000500679016\n",
      "Acc: 0.35600000619888306\n",
      "Acc: 0.3700000047683716\n",
      "Acc: 0.3869999945163727\n",
      "Acc: 0.4050000011920929\n",
      "Acc: 0.3840000033378601\n",
      "Acc: 0.40400001406669617\n",
      "Acc: 0.41999998688697815\n",
      "Acc: 0.42100000381469727\n"
     ]
    }
   ],
   "source": [
    "m = demo_model(use_elliptical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.3540000021457672\n",
      "Acc: 0.3160000145435333\n",
      "Acc: 0.3109999895095825\n",
      "Acc: 0.34700000286102295\n",
      "Acc: 0.34700000286102295\n",
      "Acc: 0.34200000762939453\n",
      "Acc: 0.33500000834465027\n",
      "Acc: 0.33899998664855957\n",
      "Acc: 0.34700000286102295\n",
      "Acc: 0.3490000069141388\n"
     ]
    }
   ],
   "source": [
    "m = demo_model(use_elliptical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(use_elliptical=False):\n",
    "    m = build_model(use_elliptical)\n",
    "    preds = m(train_x)\n",
    "    print(\"Mean activation size last layer:\", preds.abs().mean())\n",
    "    print(\"--> std:\", preds.abs().std())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss = loss_func(preds, train_y)\n",
    "    loss.backward()\n",
    "    linear_terms_grad = torch.cat([layer.weight.grad.flatten() \n",
    "                                   for layer in m if hasattr(layer, 'weight')])\n",
    "    print(\"Mean gradient size of linear terms' weights:\", linear_terms_grad.abs().mean())\n",
    "    print(\"--> std:\", linear_terms_grad.abs().std())    \n",
    "    quadratic_terms_grad = [layer._quadratic.weight.grad.flatten() \n",
    "                            for layer in m if hasattr(layer, '_quadratic')]\n",
    "    if len(quadratic_terms_grad) > 0:\n",
    "        quadratic_terms_grad = torch.cat(quadratic_terms_grad)\n",
    "        print(\"Mean gradient size of quadratic terms' weights:\", quadratic_terms_grad.abs().mean())\n",
    "        print(\"--> std:\", quadratic_terms_grad.abs().std())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean activation size last layer: tensor(0.3503, grad_fn=<MeanBackward0>)\n",
      "--> std: tensor(0.2829, grad_fn=<StdBackward0>)\n",
      "Mean gradient size of linear terms' weights: tensor(0.1711)\n",
      "--> std: tensor(0.4079)\n"
     ]
    }
   ],
   "source": [
    "report_stats(use_elliptical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean activation size last layer: tensor(0.5348, grad_fn=<MeanBackward0>)\n",
      "--> std: tensor(0.4389, grad_fn=<StdBackward0>)\n",
      "Mean gradient size of linear terms' weights: tensor(9.6714)\n",
      "--> std: tensor(46.4385)\n",
      "Mean gradient size of quadratic terms' weights: tensor(1.1180)\n",
      "--> std: tensor(4.4834)\n"
     ]
    }
   ],
   "source": [
    "report_stats(use_elliptical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fitting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda-env-newlogic-py",
   "language": "python",
   "name": "conda-env-newlogic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
