{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the trainability of a deep ReLog network (using an old formulation: $relog = log_n(x+1/n)+1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "train_x = torch.randn(1000, n)\n",
    "train_y = torch.randint(3, size=(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_strength = 0.5\n",
    "\n",
    "class ReLog(nn.Module):\n",
    "    def __init__(self, n=10):\n",
    "        super(ReLog, self).__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, input):\n",
    "        effective_log_strength = max(0, log_strength)\n",
    "        linear_term = F.relu(input)\n",
    "        relog_func = lambda x: torch.log(F.relu(x) + 1/self.n) / math.log(self.n) + 1\n",
    "        log_term = relog_func(input + effective_log_strength)\n",
    "        return log_term * effective_log_strength + linear_term * (1-effective_log_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(use_relog=False):\n",
    "    layers = []\n",
    "    for _ in range(20):\n",
    "        layers.extend([\n",
    "            nn.Linear(n, n),\n",
    "            ReLog() if use_relog else nn.ReLU(),\n",
    "            nn.BatchNorm1d(n)\n",
    "        ])\n",
    "    return nn.Sequential(*(layers + [nn.Linear(n, 3)]))\n",
    "    \n",
    "def demo_model(use_relog=False, n_epochs=5, lr=0.01, report_interval=1):\n",
    "    model = build_model(use_relog)\n",
    "    # train\n",
    "    lr = 0.5\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(train_x)\n",
    "        loss = loss_func(preds, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if report_interval > 0 and epoch % report_interval == 0:\n",
    "            _, pred_y = torch.max(preds, dim=1)\n",
    "            print('Acc:', (pred_y == train_y).float().mean().item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.30300000309944153\n",
      "Acc: 0.33899998664855957\n",
      "Acc: 0.3109999895095825\n",
      "Acc: 0.3140000104904175\n",
      "Acc: 0.3070000112056732\n"
     ]
    }
   ],
   "source": [
    "relu_model = demo_model(use_relog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.32199999690055847\n",
      "Acc: 0.35499998927116394\n",
      "Acc: 0.2939999997615814\n",
      "Acc: 0.3610000014305115\n",
      "Acc: 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "relog_model = demo_model(use_relog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(use_relog=False):\n",
    "    m = build_model(use_relog)\n",
    "    preds = m(train_x)\n",
    "    print(\"Mean activation size last layer:\", preds.abs().mean())\n",
    "    print(\"--> std:\", preds.abs().std())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss = loss_func(preds, train_y)\n",
    "    loss.backward()\n",
    "    weight_grad = torch.cat([layer.weight.grad.flatten() \n",
    "                             for layer in m if hasattr(layer, 'weight')])\n",
    "    print(\"Mean gradient size of weights:\", weight_grad.abs().mean())\n",
    "    print(\"--> std:\", weight_grad.abs().std())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean activation size last layer: tensor(0.4362, grad_fn=<MeanBackward0>)\n",
      "--> std: tensor(0.3328, grad_fn=<StdBackward0>)\n",
      "Mean gradient size of weights: tensor(0.1243)\n",
      "--> std: tensor(0.2441)\n"
     ]
    }
   ],
   "source": [
    "report_stats(relu_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean activation size last layer: tensor(0.4737, grad_fn=<MeanBackward0>)\n",
      "--> std: tensor(0.3555, grad_fn=<StdBackward0>)\n",
      "Mean gradient size of weights: tensor(0.1290)\n",
      "--> std: tensor(0.2552)\n"
     ]
    }
   ],
   "source": [
    "report_stats(relog_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-newlogic-py",
   "language": "python",
   "name": "conda-env-newlogic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
